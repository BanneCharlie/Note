# JDK

*接口和抽象类的区别 :*

- 访问修饰符   抽象类具有四种访问修饰符    接口仅仅具有public
- 方法的定义   抽象类可以定义各种方法       接口仅有抽象方法
- 职责   抽象类是代码的复用  接口为制定规范  (模版方法模式)
- 单继承  多实现 

*RPC接口返回中,使用包装类型的原因 :*

- 比如某个字段表示费率Float rate,在接口返回时,如果出现异常可能返回默认值,float返回为0.0  Float类型返回是null
- 在接口中为了避免发生歧义,建议使用对象,因为他的默认值是null 出现null的时候,我们明确知道他是出错的 但是看到0.0的时候,无法确定是否真的出错;

*泛型的优缺点 :* 泛型是JDK5提供的一种新的特性,允许在定义类和接口的时候使用参数类型;声明的类型参数在使用时具体的类型来替换

- 提高代码的复用性;比如以List接口为例,将String Integer等类型放入到List中.不使用泛型,存放String类型要写一个List接口
- 安全性: 在泛型出现之前,类型转换之前需要运行时检查,类型出错程序直接GG 泛型会在编译时做类型的检查,无疑增加程序的安全性
- Java中的泛型通过类型擦除的方式实现, Java的泛型只存在编译期,Jvm是不会感知到泛型的

*反射机制 :*

- Java在运行时通过类的名称能够获取自身的信息;通过反射机制获取成员属性和方法 创建类的对象并调用这些方法
- 但是在业务代码减少反射机制的使用
  - 反射机制涉及动态解析的类型,不能执行某些Java虚拟机优化
  - 使用反射机制,参数需要包装成Object[] 类型,真正执行的时候,需要进一步的拆包成真正的类型,这一过程不仅消耗时间还会产生很多对象,对象一多就容易导致GC
  - 反射调用方法时会从数组中遍历查找,并且会检查可见性,动作都是耗费时间的 同时参数也需要进行额外的检查
- 动态代理  ORM框架  Mybatis框架    Spring框架的IOC/DI注入  JDBC的class.forName方法

*动态代理机制  :*

- JDK动态代理 通过Java.lang.reflect包中的 Proxy类和InvocationHandler接口提供了生成动态代理类的能力
  - 动态代理的对象需要一个或者多个接口
- CGLib动态代理 第三方代码生成库,运行时在内存中动态生成一个子类对象从而实现对目标对象功能的扩展
  - 运行时期可扩展Java类和实现Java接口,且无需实现接口 通过继承的方式实现对目标对象功能的扩展,达到代理类无侵入

*注解 :*

- Java中的注解为Java代码提供了元数据.注解不会影响代码的执行 它可以说为一种标识,标识类或者一个字段
- 常用于 : 反射 AOP结合起来使用   自定义注解 + AOP 实现记录日志操作     自定义注解 + 参数校验工具类 + AOP实现参数的校验

*浅拷贝和深拷贝的区别 :*

- 浅拷贝 仅仅复制对象的地址,而不是对象的本身;也就是说,原始对象和复制对象实际上共享同一个内存地址
- 深拷贝  将会重现创建一个对象,将一个对象的所有基本属性和子对象拷贝到另一个对象中;
  - 实现Cloneable接口,重写Clone()方法
  - 序列化实现深拷贝 先把对象序列化成流,再从流中反序列化成对象,这样就一定为新对象

*String字符串不可变原因 :*

- String类被final修饰 无法被继承,方法不会被覆盖
- final 修饰字符串内容的char[] 一但被初始化无法修改
- String类没有提供用于修改的字符串内容的公共方法;



*Java中的定时调度器 Timer :* 用于在指定的时间点执行任务

- TaskQueue: 任务队列,用于存储已经计划的定时任务;任务队列按照任务的执行时间进行排序,确保最早执行的任务排在队伍前面
- TimerThread: Timer内部的后台线程,他负责扫描 TaskQueue 中的任务,检查任务的执行时间,然后在执行时间到达时执行任务run()方法,TimerThread是一个守护线程,因此所有非守护线程完成时,它就会终止;

# Collection

![image-20240319155650953](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240319155650953.png) 

## ArrayList

*ArrayList 底层原理 :*  有序可重复数据,线程不安全    可以存放为NULL的数据

- ArrayList通过动态数组进行实现的 
- 不指定集合容量,默认为10  指定集合容量根据指定容量
- ArrayList每次添加数据都会 通过size + 1 去比较容量,不满足容量将会进行1.5被扩容 

*LinkedList 底层原理 :* 有序可重复数据,线程不安全  可以存放NULL数据

- LinkedList通过双向链表进行实现
- 并且标记头尾指针 每个位置存储 前置和后置指针+存储的数据

*ArrayList 和 LinkedList的区别 :*

- 底层存储的数据结构不同
- 占据的空间不同
  - ArrayList 占据连续的内存空间,内存相较于占据较少
  - LinkedList 占据非连续的空间,且存储 前后指针 + 数据 内存占据相较于较大
- 效率
  - ArrayList 对于通过索引进行查询效率较高 O(1)  非索查询效率 O(n)
  - LinkedList 查询头尾数据为O(1)  其余查询效率为O(n)
  - ArrayList插入和删除数据 除尾部进行效率O(1) 其余全为 O(n)
  - LinkedList 插入和删除数据 头部和尾部效率O(1)  其余全为O(n)
- 线程安全  
  - 都是线程不安全的   成为线程安全  --> 方法内部使用,定义为局部变量  /  通过使用Collections.synchronizedList()方法创建

## HashMap

*HashMap 的底层原理 :*  底层原理为 Hash表数据结构 即数组和链表或红黑树(防止恶意攻击)  负载因子为0.75

- 当向HashMap中put元素时,利用key的hashCode计算的数据%数组的容量计算出当前对象的元素在数组中的下标
- 存储时,出现hash值相同的key,此时具有两种情况
  - key值相同,则覆盖原始值
  - key值不同(出现冲突),则将当前的key-value放入链表或红黑树中
- 当链表的长度大于8且数组长度大于 64时将转换为红黑树 当链表的长度小于6的时候,红黑树将会退化成单链表;
- 扩容阈值 = 数组容量 * 负载因子  默认数组容量 16

*HashMap 的put实现 :*

- 根据键值对数组table判断是否为空,否则进行初始化 初始化容量为16  负载因子为0.75
- 根据键值key计算hash值 对数组容量取模计算出索引值
- 判断数组的当前索引是否存在节点,不存在直接添加
- 当前数组的索引存在索引判断三种条件
  - 首元素是否和key值一样  直接覆盖
  - 判断是否为红黑树 红黑树直接添加
  - 如果为链表则进行循环在尾部添加,判断链表长度是否大于8,大于8将链表转换为红黑树
- 插入成功,判断实际容量是否超过阈值(容量值 * 负载因子)

*HashMap 的扩容机制 :*

- 第一次添加数据初始化时数组长度,默认为16 以后每次扩容都是达到阈值 (array.length() * 0.75) 进行扩容
- 初始化时设置容量, 桶数组的大小大于或等于这个初始化容量,并且为2的幂次方
- 达到扩容阈值的时候, 为扩容之前容量的2倍
- 扩容之后, 会重新计算所有元素的哈希值, 然后重新分配到新的桶中
  - 没hash冲突的节点, 通过 hash & (newCap - 1) 计算新数组的索引位置
  - 存在hash冲突的节点
    - 红黑树直接走红黑树的添加
    - 链表, 需要遍历链表, 可能需要拆分链表, hash & oldCap == 0 停留在原来的位置
    - 不满足移动到 原来位置 + 增加的数组大小  

*HashMap的寻址算法 :*

- 实现计算对象的 hashCode()  进行二次哈希为了减少hash冲突
- 将hashCode() >> 16 再进行异或运算,让hash分布更加均匀
- 最后通过 hash & (capacity - 1) 提升性能,计算索引时效率较高(capacity容量为2^n)   hash & oldCap 保留原来位置,否则新位置 = 旧位置 + oldCap 



## ConcurrentHashMap

*ConcurrentHashMap的底层原理 :*

- JDK1.7 中ConcurrentHashMap 保证线程安全原理  --> 分段锁  ReentrantLock  +  CAS
  - ConcurrentHashMap内部使用一个数组, 数组被分成多个 Segment, 每个段就是对应独立的哈希表, 想向哈希表中插入数据需要获取ReentrantLock获取不到通过CAS进行重试, 这种方式降低锁的粒度;
  - 多线程访问 ConcurrentHashMap 时, 只有访问相同的 Segment 的线程才会争夺锁; 

![image-20240403100252020](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240403100252020.png) 

- JDK1.8 中 ConcurrentHashMap 保证线程的安全原理 --> CAS + Synchronized
  - ConcurrentHashMap 使用 CAS 来保证数组节点的添加
  - ConcurrentHashMap 使用锁分段技术, 将整个数组分割为多个段(Segment) 当对某个段进行操作时, 只需要通过Synchronized锁定该段的头节点, 而无需锁定整个ConcurrentHashMap,提升了并发的性能;

![image-20240403100358861](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240403100358861.png) 

# IO

*用户空间和内核空间 :*

- 进程的寻址空间会划分为两部分: 内核空间 用户空间
- 用户空间只能执行受限命令 (Ring3) 不能直接调用系统资源, 必须通过内核空间的接口来进行访问
- 内核空间可以执行特权命令 (Ring0) 调用一切系统资源

![image-20240411140421322](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240411140421322.png) 

`五种IO模型 :` 

- 阻塞 IO 
- 非阻塞 IO (NIO)
- IO多路复用 ( NIO )
- 信号驱动IO (Signal Driven IO)
- 异步IO (Asynchronous IO) 

## Blocking IO

简介 : 阻塞IO模型中,用户进程在两个阶段(`等待数据传输阶段`--> 用户应用  `数据拷贝阶段`--> 内核  )都是阻塞的;

![image-20240407120836235](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240407120836235.png) 

## NonBlocking IO

简介 : 非阻塞IO模型中,用户进程在第一个阶段(等待数据传输阶段)是非阻塞的 (循环执行对CPU有一定的影响) , 在第二个阶段(数据拷贝)是阻塞的;

![image-20240407121821861](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240407121821861.png) 

## Multiplexed IO

简介 : 多路复用模型,以Linux系统为案例,单个线程来同时select监听多个FD(文件描述符),并在某个FD可读 可写时得到通知,从而避免无效的等待,充分利用CPU资源;

*通知方式 :*

- select   通过监视一组文件描述符(FD)的可读 可写 异常事件, 并在其中至少有一个文件进行就绪状态就唤醒进程, 使得进程可以执行相应的I/O操作
  - select 需要将文件描述符集合从用户态拷贝到内核态, 同时具有文件描述符数量的限制 1024个; 
  - 当出现满足条件的文件描述符 将整个集合从内核态拷贝到用户态; 


![image-20240407124829075](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240407124829075.png) 

- poll  通过监视一组文件描述符的就绪事件来唤醒进程
  - poll 函数在内核态中维护了一个链表, 通过遍历链表来查找就绪文件的描述符, 因此性能得到提升; 

- epoll    通知用户进程FD就绪同时,就把就绪的FD写入用户空间 
  - 通过内核中的 epoll 文件描述集合(epollfd) 来监视文件描述符上的事件, 并通过回调函数来处理就绪的事件 (添加到双链表中)
  - epoll不再需要向select / poll 遍历所有的文件描述符集合来查找就绪的FD,而是通过 红黑树 + 双向链表等数据结构, 使得在大量文化描述符的情况下依然高效  红黑树(存储需要监听的FD集合)  链表(记录就绪的FD)
  - 也无需将 内核中所有FD集合拷贝到内核态  只需将双链表只中记录就绪的FD传递给用户态即可
  - 两种工作模式:
    - 水平触发(LT)  没有被处理的就就绪事件, 下次会继续通知, 直到事件被处理 于select / poll模式相似; 
    - 边缘触发(ET)  只会向用户态通知一次就绪的事件,下次不会再通知;


![image-20240407132849560](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240407132849560.png) 

![image-20240407123343516](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240407123343516.png) 

`事件通知机制 :`

![image-20240407134209731](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240407134209731.png) 

## Signal Driven IO

简介 : 信号驱动IO与内核建立 SIGIO 的信号关联并设置回调, 当内核有FD就绪时, 就会发出SIGIO信号通知用户, 期间用户应用可以执行其它业务, 无阻塞等待;

![image-20240407142832966](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240407142832966.png) 

## Async IO

![image-20240407143100404](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240407143100404.png) 

![image-20240407143150562](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240407143150562.png)

# JUC

*线程的生命周期 :*

- 创建线程  --> NEW
- start() 方法  --> 就绪状态   抢到CPU执行权 运行状态    RUNABLE
- 调用 sleep(second)  -->  TIMED_WAITING
- wait() 方法 --> WAITING
- 无法获得锁 --> BLOCKED
- 结束 --> TERMINATED 

![image-20240414195130546](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240414195130546.png) 

## ThreadPool

*ThreadPool使用原因 :*

- 降低资源的消耗  -->  创建线程和销毁线程开销较大, 使用线程池可以复用已经创建的线程, 减少线程的创建和销毁的次数  降低系统资源的消耗
- 提高相应速度 -->  任务到达, 线程池中存在线程可以立即执行任务 无需等待线程的创建
- 统一管理和监控 --> 线程池可以统一管理和监控线程的状态 执行情况 便于监控和调试

![image-20240414195312113](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240414195312113.png) 

- corePoolSize  --> 核心线程数目
- maximumPoolSize  --> 最大线程数目 (核心线程数目 + 救急线程数目)
- keepAliveTime  --> 线程活跃时间  针对救急线程,没有新的任务 线程资源就会释放
- TimeUnit  --> 线程活跃时间单位
- workQueue  --> 任务队列  当没有空闲的核心线程时, 新来的任务会加入此队列, 队列满会创建救急线程执行任务
  - LinkedBlockingQueue  
    - 默认无界  支持有界  底层为链表
    - 懒惰加载, 创建节点时添加数据
    - 入队会生成新Node  头尾各有一把锁
  - ArrayBlockingQueue
    - 强制有界限  底层为数组
    - 初始化创建Node数组
    - 一把锁
- threadFactory --> 线程工厂, 指定线程的名称 是否为守护线程等
- RejectedExecutionHandler  -->  拒绝策略    核心线程数 + 救急线程 + 队列已满
  - 直接抛出异常
  - 调用者所在的线程来执行任务
  - 丢弃阻塞队列中最靠前的任务, 放入当前任务
  - 直接丢弃任务



*如何指定核心线程数目 :*

- IO密集型:  文件读取  DB读写  网络请求  核心线程数目为 2N + 1
- CPU密集型: BitMap转换 计算型代码    核心线程数目为 N + 1



*线程池的种类有哪些 :*

- 定长线程池, 可以控制线程最大并发数, 超出的部分会在队列中等待    无界队列
- 单线程化的线程池  通过唯一的工作线程来执行任务, 保证先进先出   
- 可缓存线程池  线程池长度超过处理需要,可灵活回收空闲线程        基本上无界的核心线程数目
- 可执行延迟任务线程池  支持定期执行任务



## ThreadLocal

简介 : ThreadLocal 是java的线程封闭技术, 存储线程的局部变量, 线程之间互相隔离, 互不干扰.

`JDK8之前ThreadLocal的内部设计原理`

![image-20240329203825694](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240329203825694.png) 

- ThreadLocal创建一个ThreadLocalMap.
- Thread作为ThreadLocalMap的key,存储的局部变量作为value, 达到线程隔离的作用.

`JDK8优化之后ThreadLocal内部的设计原理`

![image-20240329204035410](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240329204035410.png) 

- 每个线程内部都有一个Map(ThreadLocalMap).

- Map里面存储的ThreadLocal对象(key)和线程变量副本(Value)也就是存储的值.

- Thread内部的Map是由ThreadLocal维护的, 有ThreadLocal负责向map获取和设置线程变量值.

- 对于不同的线程, 每次获取value(也就是副本值),别的线程并不能获取当前线程的副本值, 形成了副本的隔离,互不干扰.

# SSM

*Spring MVC的执行流程 :*

- 客户端发送请求给前端控制器接收
- 前台控制器发送请求给处理器映射器 查询handler
- 返回处理器执行链,可能包含处理器拦截器
- 前端控制器向处理器适配器发送,请求执行的handler
- 找到匹配的处理器 进行参数的处理 返回值的处理  Model and View
- 前端控制器将Model and view 发送给视图解析器 返回View对象
- 渲染视图

![image-20240318204210786](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240318204210786.png)

![image-20240318204449727](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240318204449727.png)  

*Spring的IOC和AOP :*

- 控制反转是一种设计模式,用于减少计算机程序中各个模块之间的依赖关系,我们只需要定义一个Bean的创建过程,而真正的创建 初始化 装配 生命周期都是通过容器进行管理; Spring通过依赖注入对象,我们只需要关心核心逻辑即可;
- IOC 完全符合 DIP ,高模块不直接依赖低模块,而是依赖低模块的抽象 低模块去实现抽象
- 底层原理的实现: 工厂方法设计模式



- AOP 面向切面编程   在不修改源代码的情况下,抽取并封装一个可重用的模块,可以同时作用于多个方法,减少模块耦合的同时,扩展业务功能;
- 可用于日志记录 事务处理 解决缓存
- 底层原理的实现: 动态代理模式  (JDK 根据接口进行实现    CGLIB两种框架通过继承进行实现)

*Spring的生命周期 :*

- 通过BeanDefinitionReader解析xml配置文件
- BeanDefinition获取bean的定义信息
- 调用构造函数实例化bean
- bean的依赖注入
- 处理Aware接口 (BeanNameAware BeanFactoryAware ApplicationContextAware) 获取bean对象名称 工厂等
- Bean的后置处理器BeanPostProcessor-前置
- 初始化方法(自定义的init方法)
- Bean的后置处理器后置BeanPostProcessor-后置 
- 使用Bean
- 销毁bean

![image-20240318203357738](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240318203357738.png) 

*Spring的三级缓存解决循环依赖问题 :*

1. 实例化对象A  原始对象A生成ObjectFactory对象存放在 singletonFactories
2. 需要注入对象B
3. 实例化对象B  将原始对象B生成ObjectFactory对象存放在 singletonFactories
4. 需要注入A 从三级缓存中获取A的ObjectFactory对象,创建代理对象存放到二级缓存中
5. 将A的代理对象注入给B 
6. B创建成功 加入到三级缓存中
7. 将B注入给A  
8. A创建成功 加入到三级缓存中

![image-20240318203619074](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240318203619074.png)  

*事务失效的场景 :*

- 未被Spring管理的方法 
- RunTimeException 异常未抛出, 因为事务默认对 RunTimeException 和 Error 异常进行回滚.
- 出现编译时异常, 事务也会失效, 通过添加属性可以捕获到编译时异常,并且事务不会失效      通过  **rollbackFor** 指定回滚的异常类型
- 事务注解添加位置不正确 不能添加到private 或 fianl 或 static 修饰的方法上  事务会失效  AOP也会失效
- 事务的传播行为  (REQUIRED：支持当前事务, 如果当前没有事务, 就新建一个事务)  添加事务A方法 调用 未添加事务的 B方法 在某些场景下会出现事务失效



*AOP失效的场景 :*

- 非Spring管理的Bean
- 内部调用  



*Mybatis的执行流程 :*

- 读取Mybatis配置文件: 通过配置文件加载运行环境和映射文件

![image-20240318205507477](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240318205507477.png) 

*Mybatis的缓存机制 :*



*SpringBoot的自动装配 :*

- @SpringBootApplication注解是三个注解的封装
- @SpringBootConfiguration 和 @Configuration注解相识声明当期为配置类
- @ComponentScan 注解扫描当前包以及子包
- @EnableAutoConfiguration 实现自动化配置的核心注解,该注解通过@Import注解导入对应的配置器选择器;内部读取了此项目和该项目引入的jar包类路径下META-INF/spring.factories文件中的所有配置类的全类名; 这些配置类中所定义的Bean会根据条件注解所指定的条件来决定是否导入Spring容器中

# Mysql 

*MySAM 和 InnoDB 存储引擎的区别 :*

- 事务的支持 
  - MySAM不支持事务, 且为表锁(高并发环境下可能导致性能瓶颈和数据不一致问题) 
  - InnoDB支持事务, 提供表锁和行锁, 高并发的情况下就有良好的性能
- 外键约束
  - MYSAM 不支持外键约束
  - InnoDB 支持外键约束
- 崩溃恢复
  - MySAM 不支持崩溃恢复机制
  - InnoDB 支持崩溃恢复机制 通过事务日志 (Redo Log 和 Undo Log) 来实现崩溃恢复, 保证数据崩溃时数据的一致性和完整性
- 索引存储
  - MySAM 索引文件和数据文件进行分离, 索引文件存储在 .MYI扩展名文件中  数据文件存储在 .MYD扩展名文件中
  - InnoDB 索引文件和数据文件统一存储, 存储在同一个数据文件 .ibd文件中

## Log

![image-20240330143818196](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240330143818196.png)  

*MySqL相关日志 :*

- binlog
  - binglog日志是MySQL用于记录数据库中的所有DDL语句和DML语句, 主要用来对数据库进行数据的备份, 崩溃的数据恢复和数据复制等操作

- redolog
  - redolog主要记录事务管理, 主要用来恢复数据, 也是MySql用于实现崩溃恢复和数据持久性的一种机制
  - 在事务的进行中,MySQL会将事务做了什么改动记录到redolog中,当系统发生崩溃或者异常的时候,MySQL会利用redolog中的记录信息进行恢复操作,将事务所做的修改持久化到磁盘中.
- undolog
  - undolog主要用来做数据的回滚操作.
  - MySQL会将事务修改的数据记录到undolog中,如果事务需要回滚,则会从Undo log中找到相对应的记录来撤销事务所做的修改;

---

## ACID

*事务的特性以及隔离级别 :*

事务为操作的序列,不可进行分割的工作单位 执行的这些操作要么同时成功,要么同时失败.

ACID  

- Atomicity(原子性)事务不可再分割的工作单元,要么同时成功要么同时失败   --> undolog
- Consistency(一致性) 事务执行前后,数据的完整性不会被破坏  
- Isolation(隔离性) 事务之间存在隔离级别,防止多个事务之前的互相干扰   --> 锁 / MVCC
  - 读未提交  RU    可能会出现脏读问题  --> A事务执行Insert,未commit  但是B事务读取到了A事务的Inset;
  - 读提交 RC       不会出现脏读,但会出现不可重复读问题 --> A事务第一次读取一条记录和第二次读取同一条记录内容不同(针对Update) B事务在A事务第二次读取这条记录时,进行了修改操作
  - 可重复度 RR    不会出现脏读 可重复读的问题 但会出现幻读问题 --> A事务第一次查询条 件读取到3条记录,第二次根据相同的查询结果读取到了2条记录  B事务在A事务第二次条件查询的时候,对表中的数据进行了Delete操作  导致了幻读
    - 解决幻读的方法  有两种  快照读 / 当前读
  - 串行化  不允许多个事务并发访问 
- Durability(持久性) 事务一但提交或回滚对于数据库的操作是永久性的  --> redolog

![image-20240412121802557](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240412121802557.png)  

*快照读和当前读解决幻读问题 :*

![image-20240412121848074](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240412121848074.png) 

- 快照读是普通Select查询语句的默认行为.通过MVCC来避免幻读问题
- 事务启动时,第一个查询语句执行后,会创建ReadView 后续使用的查询语句使用这个ReadView,从事务开始时的数据版本链中获取数据
- 因此整个事务中,每次查询的语句都保持一致,即使其他事务中途进行插入了新记录,也不会影响当前事务的查询结果;



- 当前读 当执行 DML 语句 和 Select ... For Update等语句,这些操作会查询到当前版本的最新数据,然后再进行下一步操作;
- 当执行Select ... For Update 操作时,条件查询到的结果集 会通过间隙锁将其锁住; 其他事务无法对锁住的结果进行插入和修改操作;

---

## MVCC

*解析MVCC的工作流程 :* MVCC并发控制机制,他通过为每个读操作创建一个视图来实现读写分离,保证多个事务同时读写同一个数据时的一致性和并发性

![image-20240412121923576](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240412121923576.png) 

![image-20240331153907564](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240331153907564.png)

- 隐藏字段 
  - TRX_ID 当前记录所属的事务id
  - TOLL_PTR 回滚指针,指向上一个数据版本
  - 隐藏主键,无主键 自动生成6个字节的rowId为主键      
- undo log  回滚日志  回滚指针的版本链为 undo log
- readview  读视图 事务进行快照读产生的视图,存储不是真正的数据,而是事务的信息
  - TRX_LIST 当前系统正在活跃的事务列表
  - UP_LIMIT_ID 活跃列表的事务最小id
  - LOW_LIMIT_ID 当前系统最大事务 + 1

*MVCC工作流程 :*

1. 执行普通 select 语句即为当前读, 读取的为当前符合可见算法的可读视图
2. 从版本链中, 读取的数据 会检查事务ID, 只有哪些未活跃(已提交 / 已回滚) 事务的数据版本才会被读取, 而且数据版本的事务ID小于当前事务ID 不是最大事务ID + 1; 确保读取的数据版本是在当前事务开始之前提交的. 
3. 隔离级别为 RR 时, 两次读取的 ReadView 保持一致, 所以来避免幻读问题

----

## Index

*索引的应用场景 :*  索引是一种数据结构 B+树的数据结构,用于提高数据库的查询效率;

- 对于数据量较大,且查询比较频繁的表建立索引;
- 常用于条件查询 排序 分组操作的字段创建索引;
- 不经常对于表中的数据进行修改操作的数据表创建索引,因为每次修改都需要重新创建索引的数据结构,会大幅度降低性能;



*查看索引是否正确使用 EXPLAIN :*

```sql
EXPLAIN select * from tb_user where id = 2;
```

![image-20240412145103895](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240412145103895.png) 

- id  查询的标识符

- select_type 查询的类型

  - SIMPLE : 简单的 SELECT 查询, 不使用 UNION或子查询
  - PRIMARY : 最外层查询
  - SUBQUERY : 子查询  DERIVED : 派生表    UNION

- type  访问类型 ☆

  - const 使用常量条件进行查询, 最多返回一行数据; 使用主键或者唯一索引
  - eq_ref 使用唯一索引或者主键进行等值查询 
  - ref 通过普通索引进行等值查询
  - range 在使用索引范围进行查询时使用
  - index 全表扫描, 只使用索引   非聚簇索引进行索引覆盖
  - all 全表扫描

- possible_keys 可能使用的索引列表    key 实际使用的索引  ☆  key_len 索引的长度 ☆  ref 索引的那一列与表的那一列进行比较  

  rows 需要检查的行数   filtered 结果集过滤的百分比   

- Extra 额外信息 ☆

  - Using index 表示使用了覆盖索引, 不需要进行回表
  - Using where 查询使用了where条件进行过滤
  - Using temporary 插叙执行过程中的临时表
  - Using filesort 表示查询需要对结果进行排序



*聚簇索引和非聚簇索引 :*   回表   减少回表的操作  -->   覆盖索引  

- 聚簇索引一般为表的主键,没有表的组件将会用唯一字段代替 如果都没有InnoDB将会自动生成一个6个字节的rowId作为隐藏的聚簇索引.
- 聚簇索引将索引和数一起存储,据根据索引字段进行排序,并将完整的数据存放到叶子节点上.



- 非聚簇索引,将索引和数据分开存储 叶子结点上存储的数据为对应的主键;
- 根据查询的数据,判断是否需要,通过获得的主键进行回表操作;



*覆盖索引 :*  减少回表

- 通过索引进行条件查询,返回的数据在索引中可以获取,无需进行回表的操作;
- 优化 : 查询的字段都包含在索引中, 因此查询时可以直接从索引中获取数据, 避免了回表带来的额外 I/O操作和数据传输.

*索引下推 :* 减少回表

- 在查询过程中尽早地使用索引来过滤数据, 减少不必要的数据传输和处理 (减少回表)
  - 传统查询处理是先通过索引定位到符合条件的行, 然后根据查询条件进一步的过滤数据;	
  - 索引下推允许数据库系统在索引中尽可能早地使用查询条件, 从而减少要检索和处理的数据量;

*最左前缀原则 :*

- 创建组合索引的顺序为 (A,B,C)  需要保证索引的正常使用,在查询条件时的字段A需要位于索引的最左则
- 最左前缀原则是指使用联合索引时, 索引的前缀可以被用于查询优化;   
- 比如: 联合索引(A, B, C) 查询时最先根据最左边的列 A 进行查询优化, 因为联合索引在 B+树的创建时, 根据先 A 后 B 再 C 进行排序的. 若没有 A的字段, 就会出发索引失效的情况, 必须要满足最左前缀原则. 



*索引失效情况 :*

- 索引字段参与运算操作,原因为参加运算操作的字段数据类型发生改变导致索引失效
- 字符串类型的字段没有添加'',字段的数据类型发生改变导致索引失效
- 进行模糊查询时 %位于最左侧,当前创建索引的字段无法进行排序比较,索引失效
- 违背了最左前缀原则

----

## SQL Optimization

*sql优化策略 :*

- 表的设计优化
  - 设置合适的数值(tinyint int bigint)
  - 设置合适的字符串类型 char 和 varchar,char的效率比较高 varchar效率稍低 
- 索引的优化  (遵守最左前缀原则 覆盖索引 使用联合索引 创建索引的字段具有唯一性)
- Sql语句优化
  - Select语句务必指定字段的名称
  - DQL语句避免索引失效的写法
  - 多表连接的优化,在业务场景允许的情况下 最好使用内连接 而不是外连接,如果使用尽量使用小表为驱动,内连接会对两个表进行优化,优先把小表放到外面,大表放到里面;小表放在外面可以减少回表的次数,大表减少过滤的数据量; 
- 主从复制 读写分离
  - 数据库读取数据的操作较多时,可以采用读写分离的架构;读写分离解决了数据库的写入影响查询效率;
    - 基于binlog实现同步,主库产生的binlog日志记录了所有的写操作,从表通过binlog日志实现同步
    - 延迟消费 避免从库获取到未同步的数据,可以设置一个延迟的时间等待一段时间后再进行数据的处理
    - 监控系统,实时监控主从库的同步状态
- 分库分表
  - 水平分库,将一个库的数据拆分到多个库中,解决海量数据存储和高并发的问题   访问需要通过 myCat(中间件进行实现)
  - 水平分表,解决单表存储和性能的问题
  - 垂直分库 根据业务进行拆分,高并发下提高磁盘IO和网络连接数
  - 垂直分表 冷热数据隔离,多表互相不影响

----

## Lock

![image-20240412200231245](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240412200231245.png) 

*对数据的操作类型划分 :*

- 读锁 / 共享锁  Shared Lock
  - 针对同一份数据, 多个事务的读操作可以同时进行而不会互相影响, 相互不阻塞.   -->  Lock接口中的读写锁的 readLock()
- 写锁 / 排他锁 
  - 当前事务写操作没有完成提交前, 其他事务无法进行任何读写操作  -->  Lock接口中的 ReentrantLock 和 synchronized 关键字

*锁的粒度进行划分 :*

```sql
# 添加锁
LOCK TABLES table_name [READ | WRITE];

# 释放锁
UNLOCK TABLES;
```

- 表级锁
  - 表级锁的读写锁   
  - 意向锁 (表锁与行锁进行共存, 存储引擎进行维护)
    - 当表中的某个行添加了行锁时, 该表就会添加意向锁(告诉其他事务已经锁定了该表中的某些记录), 此时其他事务就无法为当前表添加 表级别的读写锁, 无需再遍历一遍记录查看是否存在行级锁;
  - 自增锁   当前包含AUTO_INCREAMENT列的表中插入数据时, 需要获取的一种特殊的表级锁
    - 事务持有AUTO_INC锁的过程中, 其他事务的插入语句都会被阻塞, 可以保证一个语句中分配的递增值是连续的. 
    - 当我们向一个有AUTO_INCREMENT关键字的主键插入值的时候，每条语句都要对这个表锁进行竞争，这样的并发潜力其实是很低下的. 
    - innodb通过`innodb_autoinc_lock_mode`的不同取值来提供不同的锁定机制, 提升SQL语句的可伸缩和性能.
  - 元数据锁
    - 对表做增删改查操作的时候, 加MDL读锁. 对表的结构变更操作的时候, 加MDL写锁.   

```sql
# 行级别 写锁   --> 记录锁
SELECT * FROM table_name WHERE conditions FOR UPDATE;

# 行级别 读锁  
SELECT * FROM table_name WHERE conditions LOCK IN SHARE MODE;
```

- 行级锁

  ```sql
  # 添加 记录锁
  insert update delete  select ... for update;
  
  # 添加 间隙锁 + 记录锁 --> 临键锁
  select * from user where  id > 2 and id < 10 for update; 
  ```

  - 记录锁  
    - 记录锁锁住表中单个行记录, 当前事务进行记录修改时 (Update  Delete), Mysql会自动给当前行添加上记录锁, 防止其他事务同时进行修改该行记录, 从而保证数据的一致性和完整性.
  - 间隙锁  通过索引进行查询时, 自动添加行级锁    
    - 锁定索引范围的特殊行级锁, 当前事务对索引范围内的行记录进行查询时, Mysql会自动给该索引范围加上间隙锁, 防止其他事务在该范围内插入新的行记录, 从而保证查询结果的一致性, 防止幻读.
  - Next-key 锁  记录锁 + 间隙锁
    - 锁定了索引范围 + 锁定了索引范围内的行记录.
  - 插入意向锁
    - 插入意向锁是用于控制并发插入操作的一种特殊行级锁; 当一个事务要向表中插入新的行记录, MySQL 会自动给表的插入位置加上插入意向锁，防止其他事务同时插入新的行记录, 从而避免插入冲突; 
    - 插入意向锁并不是严格的锁, 它仅仅表示事务有意向在表中插入新的行记录, 不会影响其他事务对表的读取或写入操作.

- 页级锁

*态度进行划分 :*

- 乐观锁  认为同一数据的并发操作总不会发生, 属于小概率事件, 不需要每次进行上锁, 更新时判断其他事务是否修改了此数据
  - 不通过数据库自身的锁机制, 通过程序来实现, 基本不会发生阻塞状态, 类似于 CAS机制 适用于多读的场景, 提高吞吐量
    - 版本号  对比版本号是否发生变化
    - 时间戳  当前的时间戳和更新之前的时间戳
- 悲观锁 认为同一数据总是要进行写操作, 事务需要先获取到锁, 才能进行下一步操作, 这样其他事务就会被阻塞, 性能不高但是安全性较好 适合写操作较多的场景.

*加锁方式进行划分 :*

- 显示锁  通过特定的语句进行加锁，我们一般称之为显示加锁
- 隐式锁  程语言或者框架自动进行管理和控制  synchronized  

*全局锁 :*

```sql
# 全局读锁
Flush tables with read lock
```

- 全局锁的典型使用`场景`是：做`全库逻辑备份`  



*死锁 :*  互相占有循环等待

- 进入等待, 等待超时 直接回滚  通过 innodb_lock_wait_timeout 设置超时时间
- 发起死锁检测, 发现死锁, 主动回滚死锁链中的某一个事务(最少行级排他锁的事务回滚), 其他事务执行. `innodb_deadlock_detect`设置为on 开启
- 死锁检测的过程尤其耗费性能 需要对每个事务之间都要进行排查 建议通过优雅的方法, 减少死锁的发生.

*减少死锁发生的操作 :*

- 规定相同的顺序来访问共享资源
- 避免事务中的循环等待
- 避免锁的嵌套, 放在同一级别上; 

# Redis

*Redis为何选择单线程 :*

- 抛开持久化不谈, Redis是纯内存操作, 执行速度非常快, 他的性能瓶颈是网络延迟而不是执行速度, 因此多线程并不会带来巨大的提升.
- 多线程过多导致上下文的切换, 带来不必要的开销.
- 多线程的引入必然存在线程安全问题, 就需要锁的引出来保证安全, 实现复杂度增高.

*Redis 线程模式 :*

- 核心业务模式 ( 业务部分), 为单线程
- 整个Redis, 结果就是多线程
- Redis 中引入多线程异步任务来处理耗时比较长的任务, 例如异步删除命令 unlink  异步将快照写入磁盘bgsave
- 网络模型中引入多线程, 提高对多核CPU的利用效率.

 

## Type

*常用数据类型 :*  实际开发使用过 String类型 和 Hash类型 展开讲讲

- String类型   --> 字符串类型为最常用的类型  一般存储验证码 或者 短信验证码 或者 用户的id等  规则一般为 项目名称:业务名:UID + value
- List类型  底层为双端队列  
- Hash类型 底层为Hash表 --> key value 形式  value里面存放的为另一个key value 实际项目中我key存放商品的分类信息  value里面key存放商品id value就是值
- Set类型   --> 互相关注   用户关注的UP存放在SET集合中  SINTER 返回集合之间的交集
- SortSet类型  有序不可重复  --> 点赞功能的实现   根据时间戳进行排序
- GEO类型 (SortSet类型) --> 实现附近店铺的查询 GEOSearch 根据成员的经纬度 半径 矩形范围 圆形范围查找  返回满足成员的坐标和距离
- HyperLoglog类型  -->   统计用户访问量 (每个用户访问统计一次,不会重复统计)  基数估计算法 PFCOUNT
- BitMap类型 (String类型) --> 用于实现签到  SETBIT key offset value(0 / 1)   来记录用户的活跃度



## Persistence

*持久化的方式 :*  两种持久化的方式  RDB AOF 

- RDB
  - 将Redis内存的数据产生一个快照,满足一些条件自动保存在磁盘上,防止数据在Redis进程异常退出或服务器断电的情况下丢失;
  - 定期更新的策略默认的有 1小时修改一次就会进行保存  5分钟修改100次就会触发自动保存  还可以通过 save 或 bgsave手动更新 (不会发生阻塞)
  - 不推荐使用 save 因为他可能会出现阻塞的问题  bigsave就不会出现线程阻塞的问题,因为他的主线程会fork出一个新的分支 来进行更新操作
  - 主从复制,主节点自动触发
  - flushdb / flushall 命令会产生dump.rdb文件 但也会将命令记录到 dump.rdb文件中
- AOF  需手动开启AOF
  - 以记录日志的形式,将redis进行的操作写入文件中,无法进行修改操作 只能进行添加操作
  - 执行流程为 用户发送redis命令 交给redis redis将进行的操作写入到AOF缓存中,通过AOF缓存再写入 Cache Page中 OS根据设置的策略规则写入到 磁盘中AOF文件, AOF文件的增加会触发重写机制进行压缩文件
  - AOF三种写回策略
    - Always 同步写回: 写指令执行完毕,里面同步到磁盘中
    - Everysec 每秒写回: 每个写命令执行完毕,先放入到AOF缓冲区,每个一秒将缓冲区的数据进行写入
    - NO 操作系统控制写回  随机性太大了
- RDB + AOF  
  - 开启混合持久化,AOF重写时会把Redis中的持久化数据,以RDB的格式写入到AOF文件中,之后的数据以AOF的格式追加到文件末尾
  - 以RDB的格式开头,使Redis更快启动,同时结合AOF的优点,减少大量数据的丢失风险



*主从复制的执行流程 :*

- 从节点发送请求到主节点 请求同步数据  (replication id 、offset)
- 主节点根据从节点发送的请求信息,  判断是否是第一次与从节点同步版本信息
- 主节点执行 bgsave操作, 生成rdb文件, 发送给从节点
- 将生成rdb文件的过程中, 执行的执行添加到缓存中生成缓存文件, 发送给从节点, 实现同步. 
- 同步完成后 从机会每10s向主机发送心跳, 确保从机还在.
- 主机会将修改的命令自动发送给从机,完成同步



- 从机下线,在上线  发送请求给主节点, 主节点判断,是不是第一次请求, 不是第一次获取 从节点的offset值. 
- 通过offset值的对比后, 将从机未具有的数据进行复制到offset中, 发送给从节点; 



*哨兵节点的选举方式 :*

- 主观下线   一个哨兵会定期检查一个节点,当发现某个节点 超过规定时间内,未进行返回信息,当前哨兵认为此节点宕机;这是主观下线.  
- 客观下线    这时其他哨兵也会来检测此节点,当哨兵达成一致意见时,就会确认该节点已经宕机 若是主节点将会进行选举新的节点
- 主节点选举模式
  - 根据优先级进行选举, 越小优先级越大  可通过redis.conf文件 slave-priority 属性修改
  - 根据复制偏移量位置, 偏移量越大的为从节点
  - 根据 RUN ID 的ASCII码值进行比较 




## Redission 

*Redis 分布式锁的实现原理 :*    存在的问题 --> 无法实现锁重入  不可重试 分布式只能尝试一次(可以多出尝试)   超时释放 (获取锁的客户端在一定时间内未能完成任务, 导致锁被自动释放)  

- 利用 set nx ex 获取锁, 并为锁设置过期时间, 保存线程标识
- 释放锁时, 先进行判断线程的标识是否一致, 一致则进行锁的释放(删除锁)
- 特性: 
  - set nx 满足互斥
  - set ex 保证线程故障, 依然释放锁, 避免出现死锁的现象

简介 :  Redisson 是一个基于 Redis 的 Java 客户端, 其中就提供了 分布式对象、分布式锁、分布式集合、分布式限流器、分布式消息队列; 

*Redission 锁重入 :*

- Lock锁底层通过 voaltile修饰的变量state来记录重入的状态; synchronized,在底层中通过count变量来实现, 原理和state类似.
- Redission通过 redis中的 hash结构来存储, key值表示锁是否存在(是否被获取), field表示当前持有锁的线程, value表示重入锁的次数

![image-20240413205655529](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240413205655529.png) 

*Redission 锁重试  超时释放 :*

- 当前线程进行抢夺锁 tryAcquire()  --> 获取锁的过程是 一个 while(true) 再次触发抢夺锁  设置超时等待时间 避免一直等待
-  判断当前锁是否存在, 如果不存在, 插入一把锁, 返回null 获取锁成功
- 判断当前锁是否属于当前线程, 如果是, 则返回null 重入成功
- 以上两个条件都不满足, 进入第三个条件, 返回锁失效的时间



- Redission支持锁的自动续期功能, 在获取到锁的时候, 可以重启一个定时任务(看门狗机制), 定时更新锁的过期时间, 确保锁不会因为业务逻辑执行时间过长而被自动释放  只要此线程执行 renewExpiration() 方法表示线程没有挂掉, 看门狗就会继续续约.

*Redission 多级锁 :*  Redis中为了提高性能, 采用集群模式, 读写分离 当主节点挂了 锁还么有写入到从节点 从节点升级为主节点此时 锁就失效了

- redission提出来了MutiLock锁, 使用这把锁咱们就不使用主从了, 每个节点的地位都是一样的, 这把锁加锁的逻辑需要写入到每一个主丛节点上，只有所有的服务器都写入成功, 此时才是加锁成功;.
- 假设现在某个节点挂了, 那么他去获得锁的时候, 只要有一个节点拿不到, 都不能算是加锁成功, 就保证了加锁的可靠性.



*缓存出现的问题有哪些 :*

- 缓存穿透    查询一个不存在的数据,redis找不到 mysql也找不到,这样数据不会存放在redis中 每次请求都查询数据库 

  - 缓存空值

    - 可以将这些key对应的值设置为控制, 并添加到缓存中, 再出现查询这个key的请求时, 直接返回空值

  - 布隆过滤器    一种数据结构,用于快速检索一个元素是否可能存在于一个集合中(bit 数组)  首先将热点数据进行预热加入到布隆过滤器中

    - guava布隆过滤器 Google提供的java核心库   Redis布隆过滤器 多半通过 BitMap 数据类型    
    - 它的基本原理是利用多个hash函数,将一个元素映射成多位,然后将这些位设置为1;查询某个元素时,如果这些位置 设置为1,则认为该元素可能存在集合中( 存在 hash 冲突 ),否则肯定不存在

    ![image-20240320165730060](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240320165730060.png) 

  - 增加id的复杂程度, 避免被猜测到id 规律

- 缓存击穿  当某个key值缓存过期了同时出现大量请求去访问此key值,瞬间击穿服务器直接访问数据库,数据库处于负载的情况 

  - 异步定时更新
    - 比如一个热点数据的过期时间为1个小时,那我们就每59分钟,定时任务去更新这个热点key,并重新设置过期时间
  - 互斥锁  
    - Redis中根据key获取到的值为null时,先进行上锁然后从数据库加载,加载完毕后释放锁;若其他线程也请求key时,发现获取锁失败,则进行阻塞
  - 逻辑过期  
    - 不为key设置, 真正的过期时间  通过属性 expire 记录时间
    - 查询缓存, 判断逻辑时间是否过期
    - 过期获取到互斥锁, 开启到新的线程(使用线程池)去执行缓存中数据更新的操作, 更新完成后释放锁.  (期间其他线程获取锁失败, 直接返回过期数据)
    - 直接返回过期的数据 

- 缓存雪崩  大量缓存同时过期,或缓存宕机,所有请求去直接访问数据库,造成数据库高负载,影响性能,甚至数据库宕机 

  - 不同的过期时间
    - 给不同的key的TTL添加随机值, 避免大量key过期
  - 集群
    - 集群的搭建可以有效的避免服务器单点故障出现雪崩问题;



*过期策略 :* Redis的过期策略具有两种  

- 定期删除
  - 本质为一个定时任务, 周期性的按照抽样部分过期的key, 然后进行删除; 周期分为两种模式
    - SLOW模式    根据服务器的hz频率来执行过期key的清理, 默认为10 每秒执行10次 每个执行周期为 100ms(抽查20个key),若是超过25ms 或者过期的key大于比例的10%, 再次进行一次抽取  
    - FAST模式   两次执行的间隔不超过2ms 执行的时间不会超过1ms 过期key的比例大于10% 继续进行抽查. 抽查20个key.
  
- 惰性删除
  - 当一个key过期时,不会立即在内存中删除,而是在访问这个key的时候才会触发删除操作 Redis的被动删除策略
  
   

## Consistency

*Redis和数据库的一致性问题 :*   根据业务的需求来决定 --> AP  / CP

- 优先考虑删除缓存而不是更新缓存, 删除缓存更加简单, 而且带来一致性的问题也会更少

- 要执行延迟双删的策略    先删后更再删  AP

  - 删除缓存     可能会出现 缓存击穿  通过加锁的方式防止
  - 更新数据库
  - 再次删除缓存  避免并发出现的脏数据

  为了提高稳定性和降低对代码的侵入性,考虑把缓存的删除(更新) 做成异步化,通过MQ或者监听数据库的binlog方式来进行处理
  
- 先更后删     AP  一致性更好

  - 更新数据库
  - 通过 阿里 的组件 lCanal 可以实时地监控 MySQL 数据库的变更操作 BinLog日志, 并将这些变更同步到消息队列( RabbitmQ  Kafka)或者其他存储中，供消费者进行消费和处理 --> 进行删除缓存   更加稳定和降低对代码的侵入性 
  - 删除缓存 




-  共享锁    排他锁  CP  一致性



*数据的淘汰策略 :*	

- 默认淘汰机制    --> 不淘汰任何Key, 内存满时不允许写入新的数据
- 随机淘汰策略     随机淘汰设置 TTL 的key   比较TTL的时间, 时间较短的进行淘汰
- LRU  最少最近使用   当前时间 - 最近一次使用时间    值越大淘汰的优先级别越高
- LFU  最少频率使用   每个Key的访问频率, 值最小淘汰的优先级别越高

 

*集群模式 :*

![image-20240320210328881](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240320210328881.png) 

# RabbitMQ

- 熟悉消息队列的应用场景，理解RabbitMQ确保消息可靠性的机制、消息确认的机制

*消息队列的常用应用场景 :*

- 异步执行
- 解耦: 每个服务都可以灵活插拔,不可替换 
- 流量削峰: 不管发布事件的流量波动多大,都通过Broker接收,订阅者可以按照自己的速度去处理事件



*RabbitMQ保证消息可靠的机制 :*

- 发布者发送消息给 Broker的交换机,交换机会自动返回Callback,来确定这条消息是否正确的发送到Broker的交换机中,确保消息的可靠性
- 交换机接收消息后会根据 路由键 发送给绑定的队列,发送成功没有任何操作 失败后会发送应答 退回机制
- 设置交换机 队列 消息的持久化
- 默认情况下消费者接收到消息后自动确认删除消息,将自动删除更改为手动确认消息,业务出现异常可不确认消息,重新进行投放或放入死信交换机中;



*RabbitMQ如何实现延迟消息 :*

- 死信交换机(被拒接的消息 无法消费的消息 过期的消息 消息队列已满的消息) + 设置消息过期时间 
  - 可能会造成对头阻塞,因为队列是先进先出每次只会判断对头消息是否过期,那么对头消息时间过长一直不过期,那么就会阻塞整个队列
- 通过插件进行实现
  - 消息不回立即进入到队列,而是现将他们保存基于Erlang开发的Mnesia数据库中,然后通过定时器去查询需要投递的消息,再把他们投递到死信交换机中
  - 最大的支持时间为 2^32 -1毫秒 大约49天,超过这个时间会被立即消费掉.
  
  

*如何防止RabbitMQ的重复消费 :*  一锁 二判 三更新  幂等性(同一操作的多次执行所产生的结果与单次执行的结果相同)

- RabbitMQ消费消息具有确认机制,默认情况下 消费者在投递消息成功后,会发送一个确认机制,消息队列接收到之后,就会将消息从队列中删除
- 可能出现网络波动的情况下,导致确认消息没有及时发送到消息队列,导致消息重投了,是有可能的 所以我们使用MQ的时候,消费者要做好幂等性的处理
- 通过每一个消息设置一个唯一性的id, 通过 reids 的 set nx 操作加入到Redis中, 下次执行同样的操作 判断消息是否已经被消费 成功未消费 失败消费过

# Spring Cloud Alibaba

## Nacos

Nacos是阿里巴巴开源的服务注册中心和配置中心,并提供了可视化的配置管理工具

![image-20240322163352560](https://banne.oss-cn-shanghai.aliyuncs.com/Java/image-20240322163352560.png) 

*Nacos的执行流程 :*

1.  将服务提供者进行 注册服务信息
2. 定时拉取服务的消费者
3. nacos主动查询服务提供者的心跳
4. 服务提供者变更时,主动推送变更的消息



- Nacos支持服务端主动检测提供者状态: 临时实例采用心跳模式(AP 高可用) 非临时模式采用主动检测模式(CP 强一致)
- 临时实例心跳不正常会被剔除 非临时实例则不会被剔除
- Nacos支持服务列表变更的消息推送模式,服务列表更新更及时

## Gateway

*实现负载均衡 :*

- 网关会与微服务注册中心进行通信,获取注册在注册中心的所有服务以及对应的信息
- 选择合适的负载均衡策略,
  - 轮询  依次将请求分配给后端的不同实例,按照顺序轮流分配
  - 加权轮询 根据后端服务的权重,决定分配请求的比例
  - 随机    随机选择一个服务
  - 最小链接 选择当前请求最少的后端服务
  - 哈希 根据某些属性计算哈希值,将相同哈希值的请求到路由到同一个后端服务实例

# Project

## 身份验证

JWT + Redis + ThreadLocal 基于 Token 身份验证

- 用户分布式登录后, 服务器验证用户信息正确生成 JWT, 将用户的非敏感信息加入到 JWT中, 将 JWT 令牌存储到 Redis中, 并设置过期时间. 然后将JWT返回给客户端
- 客户端存储 Token, 将收到的 JWT 存储在Cookie中 (localStorage), 每次发送请求会携带 JWT 返回给服务器
- 服务器验证 Token, 登录成功 验证Token 从Redis中获取Token, 进行验证
- 验证完身份后, 解密JWT 将用户的基本信息存储到 ThreadLocal中, 当前线程全局可用 并且为线程安全.



*Session 和 Cookie localStorage区别 :*

- Session数据存储到服务端  生命周期通过服务器控制, 一般在登录创建, 退出/会话超时销毁.
- Cookie 和 localStorage 存储在客户端的浏览器中, 可以长期保存    localStorage生命周期手动清除否则一直存在
- Cookie以键值对的形式存储在HTTP请求头中  生命周期关闭会话   / 持久Cookie设置过期时间
- 安全性 : Session数据存储在服务器较为安全,不容易收到XSS(跨站脚本攻击) 和 CSRF(跨站请求伪造)   Cookie 和 localStorage 容易收到攻击
- 跨域: 不同域名不能共享, 当前域名下可以共享    同一个域名下不同页面之间可以共享数据,  Cookie可以Domain属性实现,跨子域共享. 



ThreadLocal  --> 用来存储线程的局部变量的, 确保每个线程都拥有自己独立的变量副本, 这样不同线程之间的变量互不影响., 实现了线程安全. ThreadLocal + ThreadPool  出现 OOM(未及时释放) + 脏读问题(当前线程结束, 没有释放, 导致再次到来的线程拥有上个线程的ThreadLocal信息)

## 日志记录

AOP + 策略模式 + ThreadPool  

- 为需要记录日志的方法, 添加上特定的 Log注解, 注解中定义着 描述信息 + java类型, 通过 AOP 来对这些方法进一步的处理
- 定义一个日志记录策略转换接口, 针对不同类型的日志, 实现具体的日志记录策略类, 每个策略类负责处理特定类型的日志记录逻辑.
- 在记录日志时, 可以通过线程池来异步执行日志记录的操作, 提高系统的响应速度和并发性能. 

## 接口幂等性

AOP + Redis     发送一次请求和多次请求执行的结果相同( Insert / Delete )

-  为需要进行幂等性处理的方法, 添加上特定的注解, 通过AOP来对这些添加注解的方法进一步的处理
- 根据请求的唯一标识符, 作为key 存储到redis中, 并设置一定的过期时间, 每次请求来到先根据 key 判断是否存在相同的标识符.
- 根据判断的结果来决定是否要进行下面的业务处理.

##  秒杀商品 防止超卖

Redis  +  Lua + Redission +  消息队列

- 将数据库中准备秒杀的商品信息 库存数量等信息, 添加到缓存中, 通过使用Redis的原子性操作保证库存的一致性. 其中 项目名称:业务名:商品Id为 key 库存为value
- Lua脚本实现原子性扣减库存, 确保高并发情况下,不会出现超卖问题.  
- Redission实现分布式锁, 来控制对数据库库存的修改操作和并发访问, 确保线程安全.
- 消息队列处理订单, 通过异步执行来处理生成的订单, 避免秒杀接口的阻塞.



*Lua脚本保证原子性操作的原因 :*

- Redis执行 Lua脚本时, 在服务器端编译成字节码, 执行时以原子性的方式执行整个脚本.
- 在执行过程中Redis 采用单线程的模型, Redis执行Lua脚本时, 会将他作为一个整体并把它作为一个任务添加到队列中, 单线程依次执行这些任务, 不会被其他命令或请求打断.
- 同时还支持事务, 要么全部成功 要么全部回滚

## 重复消费

Redis +  唯一标识符 + Redission  --> 防止重复下单

- MQ实现下单操作, 传递唯一标识符 Id  作为key  创建分布式锁
- 获取锁成功, 判断是否存在下单完成的数据 不存在 ,进行下单操作 执行完成后将数据存入到Redis中 设置key值,释放锁
- 下一个线程 获取锁成功, 判断Redis中是否存在下单完成的数据, 存在直接返回